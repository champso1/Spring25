\section{Validation}\label{sec:6-verification-validation}

In this section, we attempt to validate the input variables and other components of our model by ensuring that they are physically consistent, and their error does not impact the performance or output of the model. We do statistical tests for the Monte Carlo algorithm and compare our results to \textsc{MadGraph5} and \textsc{Pythia8} to ensure our model is giving consistent results. We also verify the individual components of our model via unit tests to ensure that all the moving pieces of the model are performing as expected and allowing the simulation to perform without issue.


\subsection{Values of Hard-Coded Constants in the Literature}

The simplest starting point for validation is to check the hard-coded variables present in the \texttt{Constants.hpp} header file. The Particle Data Group (PDG)~\cite{PDG} contains listings for all of the values and their current known accuracy that I have used for my program. These can be found online at \href{https://pdg.lbl.gov/}{this link}. They are listed here, with comments on their precision:

\begin{itemize}
\item $N_C$, $T_R$, $C_A$, and $C_F$: These are group-theoretical constants, and are exact, i.e. not calculable or anything. They are not contained within PDG tables, but a Quantum Field Theory textbook usually contains the values for these variables; see, for instance, Chapter 16/17 of~\cite{peskin-schroeder}.
\item $\alpha$: The QED coupling constant, otherwise known as the fine-structure constant. This is calculable and known to a very high precision.
\item $m_Z$, $\Gamma_z$: The mass and width (lifetime, essentially) of the $Z$-boson. This is often used as a reference scale for many calculations.
\item $m_C$, $m_B$: The masses of the charm and bottom quarks, which are often higher than the lowest cutoff scale but lower than final evolution scales, so these are used as intermediate references in some cases.
\item ``Magic Factor'': The magic factor is a simple unit conversion factor. In our calculations, we often set $\hbar=c=1$ to simplify things, but of course, we must restore their values at the very end. This is done via this factor. As a unit conversion, there is no error associated with this value.
\item $G_F$, $\theta_W$: The Fermi coupling constant and Weinberg angle (often called the \textit{weak-mixing angle}), are values used in calculations of the cross section.
\item $\alpha_s(m_Z)$: The value of the strong coupling constant evaluated as the scale associated with the mass of the $Z$-boson. As mentioned, the $Z$-boson mass is very often used as a reference scale, and therefore so is the value of the strong coupling constant at that scale.
\end{itemize}

All of these values are \textit{constants} in the very sense of the word: they must be hard-coded, otherwise, if they were to be configurable and changed from what they are currently set to, we would be simulating non-physical processes. Importantly, all of these quantities have been calculated to a very high precision, as one can see by examining the PDG tables. Therefore, it is reasonable to assume that the error incurred on the model output by the choice of these parameters is negligible.


\subsection{Monte Carlo Convergence}

It is of interest to ensure that the Monte Carlo algorithm performs well, particularly in the residual statistical error incurred via the inherent limited number of iterations we can compute. In general, we can assess the error via standard statistical methods, such as computing the variance and standard deviation. The basic definition for the variance, denoted by $\sigma^2$, of a quantity $X$ is

\begin{equation}
  \sigma^2 = \braket{X^2} - \braket{X}^2,
\end{equation}

or, in discrete terms:

\begin{equation}
  \sigma^2 = \frac{1}{N}\sum_i X_i^2 + \left( \frac{1}{N} \sum_i X_i \right)^2.
\end{equation}

The standard deviation is simply the square root of the variance, hence it is denoted $\sigma$. By running the Monte Carlo simulation for enough iterations, we can reduce this standard deviation to within acceptable bounds. In particular, Table~\ref{tbl:cross-section-error} contains, for a few values of the center-of-mass energy $E_{\mathrm{ECM}}$, the calculated cross section and corresponding standard deviation measurements, using one million Monte Carlo evaluations.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    $E_{\mathrm{ECM}}$ & Cross Section (picobarns) & Error \\ \hline
    $\qty{12.0}{\tera\electronvolt}$ & $\qty{1541.2249}{\pico\barn}$ & $\pm 5.5898$ \\ \hline
    $\qty{13.0}{\tera\electronvolt}$ & $\qty{1685.1843}{\pico\barn}$ & $\pm 6.0952$ \\ \hline
    $\qty{14.0}{\tera\electronvolt}$ & $\qty{1829.2264}{\pico\barn}$ & $\pm 6.6145$ \\ \hline
    $\qty{15.0}{\tera\electronvolt}$ & $\qty{1972.5091}{\pico\barn}$ & $\pm 7.1462$ \\ \hline
  \end{tabular}
  \caption{Values of the cross section calculation along with standard deviations for different values of $E_{\mathrm{ECM}}$, using one million Monte Carlo evaluations.}
  \label{tbl:cross-section-error}
\end{table}

From the results, we can tell that the error is already very small for one million iterations. The code takes around 1 second to do these computations, so for comparisons sake, Table~\ref{tbl:cross-section-error2} contains the same data but instead taking ten million Monte Carlo iterations, which increase the runtime by a factor of 10 (as one would expect).


\begin{table}[h]
  \centering
  
  \begin{tabular}{|c|c|c|}
    \hline
    $E_{\mathrm{ECM}}$ & Cross Section (picobarns) & Error \\ \hline
    $\qty{12.0}{\tera\electronvolt}$ & $\qty{1550.8109}{\pico\barn}$ & $\pm 1.7763$ \\ \hline
    $\qty{13.0}{\tera\electronvolt}$ & $\qty{1687.4082}{\pico\barn}$ & $\pm 1.9312$ \\ \hline
    $\qty{14.0}{\tera\electronvolt}$ & $\qty{1823.7320}{\pico\barn}$ & $\pm 2.0855$ \\ \hline
    $\qty{15.0}{\tera\electronvolt}$ & $\qty{1963.3334}{\pico\barn}$ & $\pm 2.2437$ \\ \hline
  \end{tabular}
  \caption{Values of the cross section calculation along with standard deviations for different values of $E_{\mathrm{ECM}}$, using ten million Monte Carlo evaluations.}
  \label{tbl:cross-section-error2}
\end{table}

As one would expect, increasing the number of iterations reduces the standard deviation associated with the calculation. However, one million iterations still gives errors of around $\sim\qty{0.3}{\percent}$, which is more than good enough. There is nothing to compare to the literature in this regard, as this is a simple statistical error test.

\subsection{Comparison of Cross Section Calculation with \texorpdfstring{\textsc{MadGraph5}}{MadGraph5}}

\textsc{MadGraph5} is a tool that is used by and large for cross section calculations and hard scattering event generation. It provides a bit of a simpler interface for the direct calculation of the cross section, so for this comparison, we will use it. In Table~\ref{tbl:cross-section-comparison} are listed values from $E_{\mathrm{ECM}} = \qty{12}{\tera\electronvolt}$ to $\qty{15}{\tera\electronvolt}$, comparing the results obtained from \textsc{MadGraph5} and \textsc{ColSim}.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    $E_{\mathrm{ECM}}$ & \textsc{ColSim} $\sigma_{\mathrm{xs}}$ & \textsc{ColSim} $\sigma_{\mathrm{sd}}$ & \textsc{MadGraph5} $\sigma_{\mathrm{xs}}$ & \textsc{MadGraph5} $\sigma_{\mathrm{sd}}$ & Relative Error \\ \hline
    $\qty{12}{\tera\electronvolt}$ & $\qty{1550.8109}{\pico\barn}$ & $\pm 1.7763$ & $1553$ & $\pm 1.244$ & $\qty{0.14}{\percent}$ \\ \hline
    $\qty{13}{\tera\electronvolt}$ & $\qty{1687.4082}{\pico\barn}$ & $\pm 1.9312$ & $1689$ & $\pm 1.561$ & $\qty{0.12}{\percent}$ \\ \hline
    $\qty{14}{\tera\electronvolt}$ & $\qty{1823.7320}{\pico\barn}$ & $\pm 2.0855$ & $1832$ & $\pm 1.435$ & $\qty{0.49}{\percent}$ \\ \hline
    $\qty{15}{\tera\electronvolt}$ & $\qty{1963.3334}{\pico\barn}$ & $\pm 2.2437$ & $1972$ & $\pm 1.623$ & $\qty{0.44}{\percent}$ \\ \hline
  \end{tabular}
  \caption{Comparison of cross section calculations between \textsc{MadGraph5} and \textsc{ColSim} for selected center of mass energies.}
  \label{tbl:cross-section-comparison}
\end{table}

From the results, we can tell that this portion of the simulation holds up extremely well, with the relative error between my results and results from \textsc{MadGraph5} being less than half a percent. Higher energies start yielding larger errors, but the error is still very manageable and reflects the performance of the model.


\subsection{Comparison of Phase Space Kinematic Distributions with \texorpdfstring{\textsc{Pythia8}}{Pythia8}}

The other part of the program consists of the parton showering. \textsc{Pythia8} is one of the more popular parton showering algorithms out there. However, there is one issue, and it is that the parton showering model implemented in \textsc{ColSim} is extremely simplistic, and even after cutting out as many additional features from the \textsc{Pythia8} parton showering options, I am still unable to reduce it to a simple quark emitting gluons from some initial scale to a final scale.

Therefore, there is no reason to directly numerically compare the results from the distributions and determine any sort of relative errors. The only possible thing that can be done is a qualitative comparison of the shapes of the output distributions, and ensure that the distributions that I generate with \textsc{ColSim} match both the general structure from that of \textsc{Pythia8} as well as the intuitive physics understanding.


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\linewidth]{./res/gfx/Q-colsim.pdf}
  \includegraphics[width=0.49\linewidth]{./res/gfx/Q-pythia.pdf}
  \caption{Comparison between parton showering invariant masses between \textsc{ColSim}(left) and \textsc{Pythia8}(right).}
  \label{fig:Q}
\end{figure}

In Figure~\ref{fig:Q} I have given the plots for the distributions of the invariant mass for the generated events. There are some discrepancies, most notably, \textsc{Pythia8} contains some additional events in the higher $Q$ range. Figure~\ref{fig:y} contains the rapidity distributions for the generated events (rapidity is a measure of speed, roughly speaking). In my code, this value is normalized a bit differently, but we notice that it follows the same pattern, i.e. higher (absolute value of) rapidity events are less likely. Lastly, in Figure~\ref{fig:x1} are the momentum fractions the quark in the initial reaction contains. These are remarkably equivalent; evidently, the quarks involved in the showering process represent those with a small momentum fraction in the proton.


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\linewidth]{./res/gfx/y-colsim.pdf}
  \includegraphics[width=0.49\linewidth]{./res/gfx/y-pythia.pdf}
  \caption{Comparison between rapidity between \textsc{ColSim}(left) and \textsc{Pythia8}(right).}
  \label{fig:y}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.49\linewidth]{./res/gfx/x1-colsim.pdf}
  \includegraphics[width=0.49\linewidth]{./res/gfx/x1-pythia.pdf}
  \caption{Comparison between the momentum fraction between \textsc{ColSim}(left) and \textsc{Pythia8}(right).}
  \label{fig:x1}
\end{figure}



\subsection{Face Validation}

My understanding of this section is that half of it is meant to be comparing the model outputs with what is expected by field experts. Of course, if my model matches the behavior both quantitatively in the case of the cross section calculation and quantitatively in terms of the parton showering, and the comparisons are drawn between programs written by field experts, then I have more or less already satisfied the requirements for this section.

Further, a few implementation specifics have been done with assistance from a particle theorist here at KSU who specializes in Monte Carlo algorithms and event generators. I have nothing to cite in this regard, but considering again that the behavior of the model matches so well with current literature and existing models I think that I have satisfied this section.



\subsection{Historical Validation}

In the case of my model, there is not much of a reason to compare with historical data. In principle, the results themselves have not drastically changed, but rather our computational tools and theories have grown to more or less reduce the error/uncertainty with the actual calculation of values. For instance, the value of all the input parameters have more or less remained the same since their conception, but have simply had their uncertainty reduced. All I'd be doing, then, is comparing against results that effectively represent the incomplete theory and incomplete computational results, which is not instructive.

\section{Verification}

\subsection{Core Components Unit Tests}

Here I give some small unit tests for some of the smaller core components of the program, things like the Logger and some of the physics objects like four-vectors.

\subsubsection{Logger}

The unit tests here are minimal; all we need to test is that the four types of errors print as they should and handle the printing of various data types.

\begin{listing}
  \begin{minted}{cpp}
#include <ColSim/ColSim.hpp>
using namespace ColSim;

#include <string>
using namespace std;

int main() {

  LOGGER.logMessage("This is a message.");
  LOGGER.logWarning("This is a warning.");
  LOGGER.logError("This is an error that will NOT abort the program.");

  string str = "this is a string!";
  LOGGER.logMessage("String: %s", str.c_str());

  int x = 42;
  LOGGER.logMessage("Integer: %d", x);

  double y = 5.679122;
  LOGGER.logMessage("Double: %.5lf", y);

  LOGGER.logAbort("This is an error that will abort the program.");

  return 0;
}
  \end{minted}
  \caption{Test program to testing logging functionality.}
  \label{listing:logger-tests}
\end{listing}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\linewidth]{./res/gfx/logger-tests.png}
  \caption{Output from the code given in Listing~\ref{listing:logger-tests}}
  \label{fig:logger-tests}
\end{figure}

The code used to test the logger is given in Listing~\ref{listing:logger-tests} and the output is given in Figure~\ref{fig:logger-tests}. Evidently, the logger is performing as expected.

\subsubsection{Settings}

The Settings functionality will also be similar to test. All input variables have a default value, so the behavior for them all is that if they are neglected from the config file, no warnings/errors occur, rather the default value is set for that parameter. This can be illustrated by a simple test case as shown in Table~\ref{tbl:settings-test1}. A few variables, such as the aforementioned cutoff scales, have hard limits and cannot be configured below $1.0$, otherwise the program will crash. It also will emit warnings if the the cutoff is set too high, as this often reduces the accuracy of the calculation. These cases are given in Table~\ref{tbl:settings-test2}.

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Action} & Keeping/removing $E_{\mathrm{ECM}}$ from the config file. \\ \hline
    \textbf{Expected Output} & If omitted, keeps $\qty{14}{\TeV}$, otherwise uses specified value. \\ \hline
    \textbf{Actual Output (Omitted)} & \includegraphics[width=0.5\textwidth]{./res/gfx/settings-test1a.png} \\ \hline
    \textbf{Actual Output (Specifying $12.5$)} & \includegraphics[width=0.5\textwidth]{./res/gfx/settings-test1b.png} \\ \hline
  \end{tabular}
  \caption{Test case for omitting $E_{\mathrm{ECM}}$ from the configuration file.}
  \label{tbl:settings-test1}
\end{table}

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Action} & Specifying a value $<1.0$ and $>50.0$ for the parton evolution cutoff scale. \\ \hline
    \textbf{Expected Output} & For $<1.0$, aborts, for $>50.0$, gives warning. \\ \hline
    \textbf{Actual Output ($<1.0$)} & \includegraphics[width=0.75\textwidth]{./res/gfx/settings-test2a.png} \\ \hline
    \textbf{Actual Output ($>50.0$)} & \includegraphics[width=0.75\textwidth]{./res/gfx/settings-test2b.png} \\ \hline
  \end{tabular}
  \caption{Test case for violating ranges for parton evolution cutoff energy.}
  \label{tbl:settings-test2}
\end{table}

Evidently, the settings interface seems to be performing as expected, and aborts upon specifying parameters that are completely incompatible with the model (and underlying theory), and warns if parameters are specified that are in principle allowed, but may lead to less accurate results.


\subsubsection{Four-Vectors}

The testing for four-vectors is relatively simple: the only non-trivial functionality of four-vectors is to ensure that the calculation of the transverse momentum components, i.e. $p_T^2 = p_x^2 + p_y^2$, is performed correctly, the calculation of the norm $p_\mu p^\mu \equiv p_0^2 - p_x^2 - p_y^2 - p_z^2$ is performed correctly, and boosts along the $z$-axis (the beam axis). A boost is defined by a parameter $\beta$ which is the fraction of the speed of light to boost the particle. Defining

\begin{equation}
  \gamma = \sqrt{\frac{1}{1-\beta^2}},
\end{equation}

the boost is given by

\begin{equation}
  p^\mu \rightarrow p^{\mu\prime} = \begin{pmatrix}\gamma (p_0 - \beta p_z) & p_x & p_y & \gamma(p_z - \beta p_0)\end{pmatrix}.
\end{equation}

We choose some random values for the components:

\begin{equation}
  p^\mu = \begin{pmatrix}89.0 & 5.9 & 3.4 & 9.8\end{pmatrix}.
\end{equation}

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    Quantity & Expected & Actual \\ \hline
    $p_T^2$ & $7778.59$ & $7778.59$ \\ \hline
    $p_\mu p^\mu$ & $46.37$ & $46.37$ \\ \hline
    Boost of $\beta = 0.5$ & $p^\mu = (42.05,\ 5.9,\ 3.4,\ 412.85)$ & $p^\mu = (42.05,\ 5.9,\ 3.4,\ 412.85)$ \\ \hline
  \end{tabular}
  \caption{Calculated and expected values for calculated quantities involving four-vectors.}
  \label{tbl:four-vector-tests}
\end{table}

Table~\ref{tbl:four-vector-tests} contains the result from this test. Clearly, we are computing the correct values for four-vectors.

\subsubsection{Gnuplot}

The interface for plotting things with Gnuplot is less of a concrete test but rather that it can handle plotting some generic function. Due to the histogram format of the plotting that is done for the rest of the program, I do a mini Monte-Carlo to essentially plot $sin^2(x)$ between $0-\pi$. The listing is a little long, so it given as Listing~\ref{listing:gnuplot-testing} in Appendix~\ref{sec:gnuplot-listing}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\linewidth]{./res/gfx/sin2_x.pdf}
  \caption{Histogram form for the plot of $sin^2(x)$.}
  \label{fig:sin2x}
\end{figure}

The plot for the resulting distribution is given in Figure~\ref{fig:sin2x}. The code for the Gnuplot interface therefore works as expected, and is relatively easy to use.





\subsection{Monte Carlo Unit Test}

A simple unit test of the Monte Carlo algorithm is conducted by running the algorithm using a few simple functions that can be calculated analytically. Since there is no reason for the algorithm to work for simple functions but fail for complex functions, I have selected a few easy functions that are easy to integrate analytically (or otherwise have exact known results):

\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    Integral & Monte Carlo Result & Error & Exact Value & Relative Error \\ \hline
    $\int_0^1\dd x \; 2x$ & $1.0002$ & $\pm \num{5.774d-4}$ & $1.0$ & $\qty{.022}{\percent}$ \\ \hline
    $4\int_0^1 \frac{\dd x}{1+x^2}$ & $3.14164$ & $\pm \num{6.435d-4}$ & $\pi$ & $\qty{1.42d-3}{\percent}$ \\ \hline
    $\int_{-\infty}^\infty \dd x\dd y\dd z \; e^{-(x^2+y^2+z^2)}$ & $5.8080$ & $\pm 0.122$ & $\pi^{3/2}$ & $\qty{6.59}{\percent}$ \\ \hline
  \end{tabular}
  \caption{Test functions to ensure Monte Carlo convergence.}
  \label{tbl:monte-carlo-tests}
\end{table}
\renewcommand{\arraystretch}{1.0}

The results are shown in Table~\ref{tbl:monte-carlo-tests}. The first two functions are simple analytic functions and they yield less than a tenth of a percent relative error between the exact values and those determined from the Monte Carlo algorithm. The third function is a Gaussian, and due to the nature of the limits being $\pm\infty$, it is much harder to accurately assess the integral with smaller numbers of iterations. There are techniques to mitigate this, but the integrals that I compute do not have this feature, so I do not think it is necessary to fix anything. It is interesting to see, though, where the Monte Carlo algorithm breaks down. Despite this, like I just mentioned, the integrals I compute are finite, meaning the algorithm's convergence is very good and it works as intended.

These are the main sub-components within the project. There are a few more super minor utility functions and other things, but these are insignificant enough to not necessarily require unit testing.

\subsection{Physics Components}

The issue with trying to unit test the physics components is that, at the end of the day, the main part of the hard scattering process revolves directly around the Monte Carlo algorithm. We have shown that that works very well. The remaining physics component is, in the context of this milestone, implementing the equations that are calculated with the Monte Carlo algorithm. Therefore, there really is no remaining unit testing to be done with respect to the hard scattering.

With respect to the parton showering, the Sudakov form factor is the main driving component. This, however, is less of a general algorithm like Monte Carlo, but instead is implemented directly for this case. Therefore, the only testing that can be done is on the entire unit as a whole, which has already been done in the previous main section.

Therefore, this concludes the unit testing for the main sections of the progam. It is evident that the individual components work well on their own and can withstand different inputs being presented.




\subsection{Regression Testing}

I am unsure what to write for this section. The behavior of the above individual components have worked from the get-go, with only interface changes; i.e. the Monte Carlo algorithm has always worked, the Gnuplot interface has always plotted the functions, with, again, only minor API changes. Therefore, there is nothing to really document for this testing case, but rather, the documentation changes will be reflected in the main program documentation.



\subsection{Edge Case Testing}

This has already been conducted in the previous section and the previous milestone, specifically in the case of the Settings testing and varying of the input parameters. Specific limits have already been tested and described thoroughly in the previous section, which are the edge cases this section refers to. Therefore, refer to that section and Milestone 6 for the edge case behavior.



\subsection{Integration Testing}

Integration testing is challenging, because any integrations of the above individual components are already present in the main output that I have already extensively discussed. Therefore, considering that the model performance overall considering both the numerical results and also the simple fact that the output is readable, nothing crashes, etc., I think the integration testing is more or less satisfied.




\subsection{Code Review}

A big (and apparently controversial) opinion of mine is that code readability is entirely subjective. If one is working on a large project with a number of collaborators, it's must consider the practices of the collaborators, but this project is one I am programming myself. Therefore, readability is entirely my own preference, and has been written from the get-go with my own best practices I have been refining throughout my career.

In terms of actual coding practices as they relate to C++ programming, I have enabled all possible warnings and extra warnings during the compilation, i.e. via the \texttt{-Wall -Wextra} flags to the \texttt{g++} compiler. These have ensured that improper (not \textit{unreadable}) code is avoided as much as possible. The current state of the code compiles in its entirety with zero warning (and of course zero errors).


\subsection{Version Control}

The code can be found on GitHub \href{https://github.com/champso1/ColSim}{here}. The version controlling has been very poor recently, as the majority of the work has been spent running tests and whatnot. Further, my version controlling via Git has not been the best in general, which is something that I have to improve on. 




%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../../FinalMilestone"
%%% End:
